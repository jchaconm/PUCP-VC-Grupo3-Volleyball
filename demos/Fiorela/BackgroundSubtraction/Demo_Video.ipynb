{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Demo-Video.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8qzxbEV6vNk",
        "outputId": "606599f4-38ad-4cc0-8dc1-1491e148c0aa"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFbxUBMZ6yzC",
        "outputId": "04529657-fb0b-44fc-be73-88e87f5fb3ec"
      },
      "source": [
        "!git clone https://github.com/huuuuusy/Mask-RCNN-Shiny.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Mask-RCNN-Shiny'...\n",
            "remote: Enumerating objects: 82, done.\u001b[K\n",
            "remote: Total 82 (delta 0), reused 0 (delta 0), pack-reused 82\u001b[K\n",
            "Unpacking objects: 100% (82/82), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enoZz9r96111"
      },
      "source": [
        "import os\n",
        "os.chdir('Mask-RCNN-Shiny')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHfZ0LwO6bx-"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This demo uses mask-RCNN to accomplish the video processing. Input an original video (suppose this video include a person or some people), this program will recognize the main character and save him/her as RGB image while other background information will be changed as grayscale. \n",
        "\n",
        "|**Demo 1**|**Demo 2**|\n",
        "| :--: | :--: |\n",
        "|![](demo/fujing.gif)|![](demo/nikki.gif)|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNphyeeJ6bx_",
        "outputId": "1136dd55-2449-414c-9951-4c266ef825dd"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "# import os\n",
        "import sys\n",
        "from samples import coco\n",
        "from mrcnn import utils\n",
        "from mrcnn import model as modellib"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdiVNsmY6byA"
      },
      "source": [
        "# Prepare Model File and Configuration Information\n",
        "\n",
        "Now load the pre-trained model data (Mask-RCNN trained by COCO dataset)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyqmmAE86byA"
      },
      "source": [
        "# Load the pre-trained model data\n",
        "ROOT_DIR = os.getcwd()\n",
        "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
        "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
        "if not os.path.exists(COCO_MODEL_PATH):\n",
        "    utils.download_trained_weights(COCO_MODEL_PATH)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlU4Agh_6byA"
      },
      "source": [
        "The original configuration information is saved in config.py file. It can be changed if necessary. \n",
        "\n",
        "It's better to use the default value, but you can also change the GPU information to suit the personal GPU well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2CUee8m6byA"
      },
      "source": [
        "# Change the config infermation\n",
        "class InferenceConfig(coco.CocoConfig):\n",
        "    GPU_COUNT = 1\n",
        "    \n",
        "    # Number of images to train with on each GPU. A 12GB GPU can typically\n",
        "    # handle 2 images of 1024x1024px.\n",
        "    # Adjust based on your GPU memory and image sizes. Use the highest\n",
        "    # number that your GPU can handle for best performance.\n",
        "    IMAGES_PER_GPU = 1\n",
        "    \n",
        "config = InferenceConfig()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARpPL3xD6byA",
        "outputId": "00805b6a-bf0f-4cd8-9216-4dc0b834b4c8"
      },
      "source": [
        "# COCO dataset object names\n",
        "model = modellib.MaskRCNN(\n",
        "    mode=\"inference\", model_dir=MODEL_DIR, config=config\n",
        ")\n",
        "model.load_weights(COCO_MODEL_PATH, by_name=True)\n",
        "class_names = [\n",
        "    'BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
        "    'bus', 'train', 'truck', 'boat', 'traffic light',\n",
        "    'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',\n",
        "    'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',\n",
        "    'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n",
        "    'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
        "    'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "    'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n",
        "    'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
        "    'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
        "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n",
        "    'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "    'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n",
        "    'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
        "    'teddy bear', 'hair drier', 'toothbrush'\n",
        "]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Mask-RCNN-Shiny/mrcnn/model.py:338: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Mask-RCNN-Shiny/mrcnn/model.py:396: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/Mask-RCNN-Shiny/mrcnn/model.py:420: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "box_ind is deprecated, use box_indices instead\n",
            "WARNING:tensorflow:From /content/Mask-RCNN-Shiny/mrcnn/model.py:716: The name tf.sets.set_intersection is deprecated. Please use tf.sets.intersection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Mask-RCNN-Shiny/mrcnn/model.py:718: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Mask-RCNN-Shiny/mrcnn/model.py:768: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzPCvaIf6byA"
      },
      "source": [
        "# Define The Image Process Function\n",
        "\n",
        "Now define two image process functions to process each frame of the input video. \n",
        "\n",
        "apply_mask is used to change the background information to grayscale.\n",
        "\n",
        "display_instances is used to show the object detection result in original image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fIDlqjD6byA"
      },
      "source": [
        "# This function is used to change the colorful background information to grayscale.\n",
        "# image[:,:,0] is the Blue channel,image[:,:,1] is the Green channel, image[:,:,2] is the Red channel\n",
        "# mask == 0 means that this pixel is not belong to the object.\n",
        "# np.where function means that if the pixel belong to background, change it to gray_image.\n",
        "# Since the gray_image is 2D, for each pixel in background, we should set 3 channels to the same value to keep the grayscale.\n",
        "\n",
        "def apply_mask(image, mask):\n",
        "    size=(646,604)\n",
        "    gray_image = np.ones(size)\n",
        "    # gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    image[:, :, 0] = np.where(\n",
        "        mask == 0,\n",
        "        gray_image[:, :],\n",
        "        image[:, :, 0]\n",
        "    )\n",
        "    image[:, :, 1] = np.where(\n",
        "        mask == 0,\n",
        "        gray_image[:, :],\n",
        "        image[:, :, 1]\n",
        "    )\n",
        "    image[:, :, 2] = np.where(\n",
        "        mask == 0,\n",
        "        gray_image[:, :],\n",
        "        image[:, :, 2]\n",
        "    )\n",
        "    return image"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYWRx4nt6byA"
      },
      "source": [
        "# This function is used to show the object detection result in original image.\n",
        "def display_instances(image, boxes, masks, ids, names, scores):\n",
        "    # max_area will save the largest object for all the detection results\n",
        "    max_area = 0\n",
        "    \n",
        "    # n_instances saves the amount of all objects\n",
        "    n_instances = boxes.shape[0]\n",
        "\n",
        "    if not n_instances:\n",
        "        print('NO INSTANCES TO DISPLAY')\n",
        "    else:\n",
        "        assert boxes.shape[0] == masks.shape[-1] == ids.shape[0]\n",
        "\n",
        "    for i in range(n_instances):\n",
        "        if not np.any(boxes[i]):\n",
        "            continue\n",
        "\n",
        "        # compute the square of each object\n",
        "        y1, x1, y2, x2 = boxes[i]\n",
        "        square = (y2 - y1) * (x2 - x1)\n",
        "\n",
        "        # use label to select person object from all the 80 classes in COCO dataset\n",
        "        label = names[ids[i]]\n",
        "        if label == 'person':\n",
        "            # save the largest object in the image as main character\n",
        "            # other people will be regarded as background\n",
        "            if square > max_area:\n",
        "                max_area = square\n",
        "                mask = masks[:, :, i]\n",
        "            else:\n",
        "                continue\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        # apply mask for the image\n",
        "        image = apply_mask(image, mask)\n",
        "        \n",
        "    return image"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZS08fgm6byA"
      },
      "source": [
        "# Process Video\n",
        "\n",
        "Now use the functions above to accomplish the video processing and save the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcpBNwxK6byA",
        "outputId": "c2fc9ef8-d373-4eb3-d387-98e0e088c034"
      },
      "source": [
        "input_video = 'cam3_63.mp4'\n",
        "capture = cv2.VideoCapture(input_video)\n",
        "\n",
        "# these 2 lines can be removed if you dont have a 1080p camera.\n",
        "# capture.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n",
        "# capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n",
        "\n",
        "# Recording Video\n",
        "fps = 60.0\n",
        "width = int(capture.get(3))\n",
        "height = int(capture.get(4))\n",
        "fcc = cv2.VideoWriter_fourcc('D', 'I', 'V', 'X')\n",
        "out = cv2.VideoWriter(\"new_video.avi\", fcc, fps, (width, height))\n",
        "\n",
        "while True:\n",
        "    ret, frame = capture.read()\n",
        "    results = model.detect([frame], verbose=0)\n",
        "    r = results[0]\n",
        "    frame = display_instances(\n",
        "        frame, r['rois'], r['masks'], r['class_ids'], class_names, r['scores']\n",
        "    )\n",
        "    # cv2.imshow('video', frame)\n",
        "\n",
        "    # Recording Video\n",
        "    out.write(frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "capture.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRaM8qdH7GZG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}